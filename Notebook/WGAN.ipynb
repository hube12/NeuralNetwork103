{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WGAN.ipynb","provenance":[],"collapsed_sections":["9GYn1dSm5vgz","bHyrLEbV5y9y","tA9hpDVu7OZ7","LCtuJva270pI","nus80uhH73iX"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ME9DJNDd6FOE","colab_type":"text"},"source":["# Workspace Settings\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"XmQEIT5lAoh7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4f23a128-c9d6-4d9a-83e9-e5212f6a97ed","executionInfo":{"status":"ok","timestamp":1571709974522,"user_tz":-120,"elapsed":3228,"user":{"displayName":"neil","photoUrl":"","userId":"09670409907443135887"}}},"source":["%cd /home\n","%rm -rf NeuralNetwork103\n","%ls"],"execution_count":69,"outputs":[{"output_type":"stream","text":["/home\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AECazrey6MOr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"ce3c5477-37c6-4eac-81a9-a7fba268327c","executionInfo":{"status":"ok","timestamp":1571709982005,"user_tz":-120,"elapsed":10700,"user":{"displayName":"neil","photoUrl":"","userId":"09670409907443135887"}}},"source":["\n","!git clone https://github.com/hube12/NeuralNetwork103\n","%cd NeuralNetwork103\n","%cd Notebook\n","%rm -rf Results\n","%mkdir Results\n","%rm -rf Models\n","%mkdir Models\n","%ls\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import numpy as np\n","from PIL import Image\n","from math import floor\n","import numpy as np\n","import time\n","from functools import partial\n","from random import random\n","#Imports for layers and models\n","import tensorflow as tf\n","from tensorflow.python.framework.ops import disable_eager_execution\n","disable_eager_execution()\n","from tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, LeakyReLU, Activation\n","from tensorflow.keras.layers import Reshape, UpSampling2D, Dropout, Flatten, Input, add, Cropping2D\n","from tensorflow.keras.models import model_from_json, Model\n","from tensorflow.keras.optimizers import Adam\n","import tensorflow.keras.backend as K\n","from tensorflow import keras\n","from tensorflow.keras.layers import Layer\n","import matplotlib.pyplot as plt\n","\n","#Config Stuff\n","im_size = 32\n","latent_size = 128\n","BATCH_SIZE = 16\n","directory = \"Flowers\"\n","n_images = 8189\n","suff = 'jpg'"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Cloning into 'NeuralNetwork103'...\n","remote: Enumerating objects: 21, done.\u001b[K\n","remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n","remote: Compressing objects:   5% (1/17)\u001b[K\rremote: Compressing objects:  11% (2/17)\u001b[K\rremote: Compressing objects:  17% (3/17)\u001b[K\rremote: Compressing objects:  23% (4/17)\u001b[K\rremote: Compressing objects:  29% (5/17)\u001b[K\rremote: Compressing objects:  35% (6/17)\u001b[K\rremote: Compressing objects:  41% (7/17)\u001b[K\rremote: Compressing objects:  47% (8/17)\u001b[K\rremote: Compressing objects:  52% (9/17)\u001b[K\rremote: Compressing objects:  58% (10/17)\u001b[K\rremote: Compressing objects:  64% (11/17)\u001b[K\rremote: Compressing objects:  70% (12/17)\u001b[K\rremote: Compressing objects:  76% (13/17)\u001b[K\rremote: Compressing objects:  82% (14/17)\u001b[K\rremote: Compressing objects:  88% (15/17)\u001b[K\rremote: Compressing objects:  94% (16/17)\u001b[K\rremote: Compressing objects: 100% (17/17)\u001b[K\rremote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 21 (delta 0), reused 7 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects:   4% (1/21)   \rUnpacking objects:   9% (2/21)   \rUnpacking objects:  14% (3/21)   \rUnpacking objects:  19% (4/21)   \rUnpacking objects:  23% (5/21)   \rUnpacking objects:  28% (6/21)   \rUnpacking objects:  33% (7/21)   \rUnpacking objects:  38% (8/21)   \rUnpacking objects:  42% (9/21)   \rUnpacking objects:  47% (10/21)   \rUnpacking objects:  52% (11/21)   \rUnpacking objects:  57% (12/21)   \rUnpacking objects:  61% (13/21)   \rUnpacking objects:  66% (14/21)   \rUnpacking objects:  71% (15/21)   \rUnpacking objects:  76% (16/21)   \rUnpacking objects:  80% (17/21)   \rUnpacking objects:  85% (18/21)   \rUnpacking objects:  90% (19/21)   \rUnpacking objects:  95% (20/21)   \rUnpacking objects: 100% (21/21)   \rUnpacking objects: 100% (21/21), done.\n","/home/NeuralNetwork103\n","/home/NeuralNetwork103/Notebook\n","\u001b[0m\u001b[01;34mModels\u001b[0m/  RBM.ipynb  \u001b[01;34mResults\u001b[0m/  WGAN.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9GYn1dSm5vgz","colab_type":"text"},"source":["# Helpers\n"]},{"cell_type":"code","metadata":{"id":"pgOduing5ryQ","colab_type":"code","colab":{}},"source":["class AdaInstanceNormalization(Layer):\n","    def __init__(self, \n","             axis=-1,\n","             momentum=0.99,\n","             epsilon=1e-3,\n","             center=True,\n","             scale=True,\n","             **kwargs):\n","        super(AdaInstanceNormalization, self).__init__(**kwargs)\n","        self.axis = axis\n","        self.momentum = momentum\n","        self.epsilon = epsilon\n","        self.center = center\n","        self.scale = scale\n","    \n","    \n","    def build(self, input_shape):\n","    \n","        dim = input_shape[0][self.axis]\n","        if dim is None:\n","            raise ValueError('Axis ' + str(self.axis) + ' of '\n","                             'input tensor should have a defined dimension '\n","                             'but the layer received an input with shape ' +\n","                             str(input_shape[0]) + '.')\n","    \n","        super(AdaInstanceNormalization, self).build(input_shape) \n","    \n","    def call(self, inputs, training=None):\n","        input_shape = K.int_shape(inputs[0])\n","        reduction_axes = list(range(0, len(input_shape)))\n","        \n","        beta = inputs[1]\n","        gamma = inputs[2]\n","\n","        if self.axis is not None:\n","            del reduction_axes[self.axis]\n","\n","        del reduction_axes[0]\n","        mean = K.mean(inputs[0], reduction_axes, keepdims=True)\n","        stddev = K.std(inputs[0], reduction_axes, keepdims=True) + self.epsilon\n","        normed = (inputs[0] - mean) / stddev\n","\n","        return normed * gamma + beta\n","    \n","    def get_config(self):\n","        config = {\n","            'axis': self.axis,\n","            'momentum': self.momentum,\n","            'epsilon': self.epsilon,\n","            'center': self.center,\n","            'scale': self.scale\n","        }\n","        base_config = super(AdaInstanceNormalization, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","    \n","    def compute_output_shape(self, input_shape):\n","    \n","        return input_shape[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bHyrLEbV5y9y","colab_type":"text"},"source":["# Code"]},{"cell_type":"markdown","metadata":{"id":"kYuwRwhu7Gkt","colab_type":"text"},"source":["Basics functions\n"]},{"cell_type":"code","metadata":{"id":"50T5GlMD7Ipl","colab_type":"code","colab":{}},"source":["#Style Z\n","def noise(n):\n","    return np.random.normal(0.0, 1.0, size = [n, latent_size])\n","\n","#Noise Sample\n","def noiseImage(n):\n","    return np.random.uniform(0.0, 1.0, size = [n, im_size, im_size, 1])\n","\n","#Get random samples from an array\n","def get_rand(array, amount):\n","    \n","    idx = np.random.randint(0, array.shape[0], amount)\n","    return array[idx]\n","\n","#Import Images Function\n","def import_images(loc, flip = True, suffix = 'png'):\n","    \n","    out = []\n","    cont = True\n","    i = 1\n","    print(\"Importing Images...\")\n","    \n","    while(cont):\n","        try:\n","            temp = Image.open(\"data/\"+loc+\"/im (\"+str(i)+\").\"+suffix+\"\").convert('RGB')\n","            temp = temp.resize((im_size, im_size), Image.BICUBIC)\n","            temp1 = np.array(temp.convert('RGB'), dtype='float32') / 255\n","            out.append(temp1)\n","            if flip:\n","                out.append(np.flip(out[-1], 1))\n","            \n","            i = i + 1\n","        except:\n","            cont = False\n","        \n","    print(str(i-1) + \" images imported.\")\n","            \n","    return np.array(out)\n","\n","def normalize(arr):\n","    return (arr - np.mean(arr)) / (np.std(arr) + 1e-7)\n","\n","#r1/r2 gradient penalty\n","def gradient_penalty_loss(y_true, y_pred,sample_weight, averaged_samples, weight):\n","    gradients = tf.keras.backend.gradients(y_pred, averaged_samples)[0]\n","    gradients_sqr = tf.keras.backend.square(gradients)\n","    gradient_penalty = tf.keras.backend.sum(gradients_sqr,\n","                              axis=np.arange(1, len(gradients_sqr.shape)))\n","    \n","    # weight * ||grad||^2\n","    # Penalize the gradient norm\n","    return tf.keras.backend.mean(gradient_penalty * weight)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tA9hpDVu7OZ7","colab_type":"text"},"source":["# Data picker"]},{"cell_type":"code","metadata":{"id":"RZQa4ivT5efX","colab_type":"code","colab":{}},"source":["#This is the REAL data generator, which can take images from disk and temporarily use them in your program.\n","#Probably could/should get optimized at some point\n","class dataGenerator(object):\n","    \n","    def __init__(self, loc, n, flip = True, suffix = 'png'):\n","        self.loc = \"data/\"+loc\n","        self.flip = flip\n","        self.suffix = suffix\n","        self.n = n\n","    \n","    def get_batch(self, amount):\n","        \n","        idx = np.random.randint(0, self.n - 1, amount) + 1\n","        out = []\n","        \n","        for i in idx:\n","            temp = Image.open(self.loc+\"/im (\"+str(i)+\").\"+self.suffix+\"\").convert('RGB')\n","            temp1 = np.array(temp.convert('RGB'), dtype='float32') / 255\n","            if self.flip and random() > 0.5:\n","                temp1 = np.flip(temp1, 1)\n","                \n","            out.append(temp1)\n","            \n","        \n","        return np.array(out)\n","class dataSet(object):\n","    \n","    def __init__(self, flip = True):\n","        self.flip = flip\n","        (self.im, _), (_, _) = keras.datasets.cifar10.load_data()\n","        self.im = np.float32(self.im) / 255\n","    \n","    def get_batch(self, amount):\n","        \n","        idx = np.random.randint(0, np.shape(self.im)[0] - 1, amount) + 1\n","        out = []\n","        \n","        for i in idx:\n","            temp1 = self.im[i]\n","            if self.flip and random() > 0.5:\n","                temp1 = np.flip(temp1, 1)\n","                \n","            out.append(temp1)\n","            \n","        \n","        return np.array(out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCtuJva270pI","colab_type":"text"},"source":["# Generator"]},{"cell_type":"code","metadata":{"id":"eKS5C1rW7kpA","colab_type":"code","colab":{}},"source":["\n","#Upsample, Convolution, AdaIN, Noise, Activation, Convolution, AdaIN, Noise, Activation\n","def g_block(inp, style, noise, fil, u = True):\n","    \n","    b = Dense(fil)(style)\n","    b = Reshape([1, 1, fil])(b)\n","    g = Dense(fil)(style)\n","    g = Reshape([1, 1, fil])(g)\n","\n","    n = Conv2D(filters = fil, kernel_size = 1, padding = 'same', kernel_initializer = 'he_normal')(noise)\n","    \n","    if u:\n","        out = UpSampling2D(interpolation = 'bilinear')(inp)\n","        out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n","    else:\n","        out = Activation('linear')(inp)\n","    \n","    out = AdaInstanceNormalization()([out, b, g])\n","    out = add([out, n])\n","    out = LeakyReLU(0.01)(out)\n","    \n","    b = Dense(fil)(style)\n","    b = Reshape([1, 1, fil])(b)\n","    g = Dense(fil)(style)\n","    g = Reshape([1, 1, fil])(g)\n","\n","    n = Conv2D(filters = fil, kernel_size = 1, padding = 'same', kernel_initializer = 'he_normal')(noise)\n","    \n","    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n","    out = AdaInstanceNormalization()([out, b, g])\n","    out = add([out, n])\n","    out = LeakyReLU(0.01)(out)\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nus80uhH73iX","colab_type":"text"},"source":["# Discriminator"]},{"cell_type":"code","metadata":{"id":"C-AEaZq77mqv","colab_type":"code","colab":{}},"source":["#Convolution, Activation, Pooling, Convolution, Activation\n","def d_block(inp, fil, p = True):\n","    \n","    route2 = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(inp)\n","    route2 = LeakyReLU(0.01)(route2)\n","    if p:\n","        route2 = AveragePooling2D()(route2)\n","    route2 = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(route2)\n","    out = LeakyReLU(0.01)(route2)\n","    \n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f_OV2UUD77T-","colab_type":"text"},"source":["# GAN model"]},{"cell_type":"code","metadata":{"id":"s1PRhHni79Ea","colab_type":"code","colab":{}},"source":["#This object holds the models\n","class GAN(object):\n","    \n","    def __init__(self, lr = 0.0001):\n","        \n","        #Models\n","        self.D = None\n","        self.G = None\n","        \n","        self.DM = None\n","        self.AM = None\n","        \n","        #Config\n","        self.LR = lr\n","        self.steps = 1\n","        \n","        #Init Models\n","        self.discriminator()\n","        self.generator()\n","        \n","    def discriminator(self):\n","        \n","        if self.D:\n","            return self.D\n","        \n","        inp = Input(shape = [im_size, im_size, 3])\n","        \n","        # Size\n","        x = d_block(inp, 16) #Size / 2\n","        x = d_block(x, 32) #Size / 4\n","        x = d_block(x, 64) #Size / 8\n","        \n","        if (im_size > 32):\n","            x = d_block(x, 128) #Size / 16\n","        \n","        if (im_size > 64):\n","            x = d_block(x, 192) #Size / 32\n","        \n","        if (im_size > 128):\n","            x = d_block(x, 256) #Size / 64\n","        \n","        if (im_size > 256):\n","            x = d_block(x, 384) #Size / 128\n","            \n","        if (im_size > 512):\n","            x = d_block(x, 512) #Size / 256\n","            \n","            \n","        x = Flatten()(x)\n","        \n","        x = Dense(128)(x)\n","        x = Activation('relu')(x)\n","        \n","        x = Dropout(rate=0.6)(x)\n","        x = Dense(1)(x)\n","        \n","        self.D = Model(inputs = inp, outputs = x)\n","        \n","        return self.D\n","    \n","    def generator(self):\n","        \n","        if self.G:\n","            return self.G\n","        \n","        #Style FC, I only used 2 fully connected layers instead of 8 for faster training\n","        inp_s = Input(shape = [latent_size])\n","        sty = Dense(512, kernel_initializer = 'he_normal')(inp_s)\n","        sty = LeakyReLU(0.1)(sty)\n","        sty = Dense(512, kernel_initializer = 'he_normal')(sty)\n","        sty = LeakyReLU(0.1)(sty)\n","        \n","        #Get the noise image and crop for each size\n","        inp_n = Input(shape = [im_size, im_size, 1])\n","        noi = [Activation('linear')(inp_n)]\n","        curr_size = im_size\n","        while curr_size > 4:\n","            curr_size = int(curr_size / 2)\n","            noi.append(Cropping2D(int(curr_size/2))(noi[-1]))\n","        \n","        #Here do the actual generation stuff\n","        inp = Input(shape = [1])\n","        x = Dense(4 * 4 * 512, kernel_initializer = 'he_normal')(inp)\n","        x = Reshape([4, 4, 512])(x)\n","        x = g_block(x, sty, noi[-1], 512, u=False)\n","        \n","        if(im_size >= 1024):\n","            x = g_block(x, sty, noi[7], 512) # Size / 64\n","        if(im_size >= 512):\n","            x = g_block(x, sty, noi[6], 384) # Size / 64\n","        if(im_size >= 256):\n","            x = g_block(x, sty, noi[5], 256) # Size / 32\n","        if(im_size >= 128):\n","            x = g_block(x, sty, noi[4], 192) # Size / 16\n","        if(im_size >= 64):\n","            x = g_block(x, sty, noi[3], 128) # Size / 8\n","            \n","        x = g_block(x, sty, noi[2], 64) # Size / 4\n","        x = g_block(x, sty, noi[1], 32) # Size / 2\n","        x = g_block(x, sty, noi[0], 16) # Size\n","        \n","        x = Conv2D(filters = 3, kernel_size = 1, padding = 'same', activation = 'sigmoid')(x)\n","        \n","        self.G = Model(inputs = [inp_s, inp_n, inp], outputs = x)\n","        \n","        return self.G\n","    \n","    def AdModel(self):\n","        \n","        #D does not update\n","        self.D.trainable = False\n","        for layer in self.D.layers:\n","            layer.trainable = False\n","        \n","        #G does update\n","        self.G.trainable = True\n","        for layer in self.G.layers:\n","            layer.trainable = True\n","        \n","        #This model is simple sequential one with inputs and outputs\n","        gi = Input(shape = [latent_size])\n","        gi2 = Input(shape = [im_size, im_size, 1])\n","        gi3 = Input(shape = [1])\n","        \n","        gf = self.G([gi, gi2, gi3])\n","        df = self.D(gf)\n","        \n","        self.AM = Model(inputs = [gi, gi2, gi3], outputs = df)\n","            \n","        self.AM.compile(optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.99, decay = 0.00001), loss = 'mse')\n","        \n","        return self.AM\n","    \n","    def DisModel(self):\n","        \n","        #D does update\n","        self.D.trainable = True\n","        for layer in self.D.layers:\n","            layer.trainable = True\n","        \n","        #G does not update\n","        self.G.trainable = False\n","        for layer in self.G.layers:\n","            layer.trainable = False\n","        \n","        # Real Pipeline\n","        ri = Input(shape = [im_size, im_size, 3])\n","        dr = self.D(ri)\n","        \n","        # Fake Pipeline\n","        gi = Input(shape = [latent_size])\n","        gi2 = Input(shape = [im_size, im_size, 1])\n","        gi3 = Input(shape = [1])\n","        gf = self.G([gi, gi2, gi3])\n","        df = self.D(gf)\n","        \n","        # Samples for gradient penalty\n","        # For r1 use real samples (ri)\n","        # For r2 use fake samples (gf)\n","        da = self.D(ri)\n","        \n","        # Model With Inputs and Outputs\n","        self.DM = Model(inputs=[ri, gi, gi2, gi3], outputs=[dr, df, da])\n","        \n","        # Create partial of gradient penalty loss\n","        # For r1, averaged_samples = ri\n","        # For r2, averaged_samples = gf\n","        # Weight of 10 typically works\n","        partial_gp_loss = partial(gradient_penalty_loss, averaged_samples = ri, weight = 5)\n","        \n","        #Compile With Corresponding Loss Functions\n","        self.DM.compile(optimizer=Adam(self.LR, beta_1 = 0, beta_2 = 0.99, decay = 0.00001), loss=['mse', 'mse', partial_gp_loss])\n","        \n","        return self.DM\n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Edr2VEOu8NYC","colab_type":"text"},"source":["# WGAN"]},{"cell_type":"code","metadata":{"id":"QLCzOuOv8PsE","colab_type":"code","colab":{}},"source":["class WGAN(object):\n","    \n","    def __init__(self, steps = -1, lr = 0.0001, silent = True, use_dataset=True):\n","        \n","        self.GAN = GAN(lr = lr)\n","        self.DisModel = self.GAN.DisModel()\n","        self.AdModel = self.GAN.AdModel()\n","        self.generator = self.GAN.generator()\n","        \n","        if steps >= 0:\n","            self.GAN.steps = steps\n","        \n","        self.lastblip = time.clock()\n","        \n","        self.noise_level = 0\n","        if use_dataset:\n","          self.im= dataSet(flip=True)\n","        #self.ImagesA = import_images(directory, True)\n","        else:\n","          self.im = dataGenerator(directory, n_images, suffix = suff, flip = True)\n","        \n","        \n","        self.silent = silent\n","\n","        #Train Generator to be in the middle, not all the way at real. Apparently works better??\n","        self.ones = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n","        self.zeros = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n","        self.nones = -self.ones\n","        \n","        self.enoise = noise(8)\n","        self.enoiseImage = noiseImage(8)\n","    \n","    def train(self):\n","        \n","        #Train Alternating\n","        a = self.train_dis()\n","        b = self.train_gen()\n","        \n","        #Print info\n","        if self.GAN.steps % 20 == 0 and not self.silent:\n","            print(\"\\n\\nRound \" + str(self.GAN.steps) + \":\")\n","            print(\"D: \" + str(a))\n","            print(\"G: \" + str(b))\n","            s = round((time.clock() - self.lastblip) * 1000) / 1000\n","            print(\"T: \" + str(s) + \" sec\")\n","            self.lastblip = time.clock()\n","            \n","            #Save Model\n","            if self.GAN.steps % 500 == 0:\n","                self.save(floor(self.GAN.steps / 10000))\n","            if self.GAN.steps % 1000 == 0:\n","                self.evaluate(floor(self.GAN.steps / 1000))\n","            \n","        \n","        self.GAN.steps = self.GAN.steps + 1\n","          \n","    def train_dis(self):\n","        \n","        #Get Data\n","        train_data = [self.im.get_batch(BATCH_SIZE), noise(BATCH_SIZE), noiseImage(BATCH_SIZE), self.ones]\n","        \n","        #Train\n","        d_loss = self.DisModel.train_on_batch(train_data, [self.ones, self.nones, self.ones])\n","        \n","        return d_loss\n","       \n","    def train_gen(self):\n","        \n","        #Train\n","        g_loss = self.AdModel.train_on_batch([noise(BATCH_SIZE), noiseImage(BATCH_SIZE), self.ones], self.zeros)\n","        \n","        return g_loss\n","    \n","    def evaluate(self, num = 0, trunc = 2.0): #8x4 images, bottom row is constant\n","        \n","        n = noise(32)\n","        n2 = noiseImage(32)\n","        \n","        im2 = self.generator.predict([n, n2, np.ones([32, 1])])\n","        im3 = self.generator.predict([self.enoise, self.enoiseImage, np.ones([8, 1])])\n","        \n","        r12 = np.concatenate(im2[:8], axis = 1)\n","        r22 = np.concatenate(im2[8:16], axis = 1)\n","        r32 = np.concatenate(im2[16:24], axis = 1)\n","        r43 = np.concatenate(im3[:8], axis = 1)\n","        \n","        c1 = np.concatenate([r12, r22, r32, r43], axis = 0)\n","        \n","        x = Image.fromarray(np.uint8(c1*255))\n","        \n","        x.save(\"Results/i\"+str(num)+\".jpg\")\n","        \n","    def evaluate2(self, s1, s2, n1, n2, num = 0, weight = 0.5):\n","        \n","        s = normalize((s2 * weight) + (s1 * (1 - weight)))\n","        n = (n2 * weight) + (n1 * (1 - weight))\n","        \n","        im2 = self.generator.predict([s, n, np.ones([32, 1])])\n","        \n","        r12 = np.concatenate(im2[:8], axis = 1)\n","        r22 = np.concatenate(im2[8:16], axis = 1)\n","        r32 = np.concatenate(im2[16:24], axis = 1)\n","        r43 = np.concatenate(im2[24:], axis = 1)\n","        \n","        c1 = np.concatenate([r12, r22, r32, r43], axis = 0)\n","        \n","        x = Image.fromarray(np.uint8(c1*255))\n","        \n","        x.save(\"Results/i\"+str(num)+\".jpg\")\n","        \n","    def evalTrunc(self, num = 0, trunc = 1.8):\n","        \n","        n = np.clip(noise(16), -trunc, trunc)\n","        n2 = noiseImage(16)\n","        \n","        im2 = self.generator.predict([n, n2, np.ones([16, 1])])\n","        \n","        r12 = np.concatenate(im2[:4], axis = 1)\n","        r22 = np.concatenate(im2[4:8], axis = 1)\n","        r32 = np.concatenate(im2[8:12], axis = 1)\n","        r43 = np.concatenate(im2[12:], axis = 1)\n","        \n","        c1 = np.concatenate([r12, r22, r32, r43], axis = 0)\n","        \n","        x = Image.fromarray(np.uint8(c1*255))\n","        \n","        x.save(\"Results/t\"+str(num)+\".jpg\")\n","        \n","    \n","    def saveModel(self, model, name, num): #Save a Model\n","        json = model.to_json()\n","        with open(\"Models/\"+name+\".json\", \"w\") as json_file:\n","            json_file.write(json)\n","            \n","        model.save_weights(\"Models/\"+name+\"_\"+str(num)+\".h5\")\n","        \n","    def loadModel(self, name, num): #Load a Model\n","        \n","        file = open(\"Models/\"+name+\".json\", 'r')\n","        json = file.read()\n","        file.close()\n","        \n","        mod = model_from_json(json, custom_objects = {'AdaInstanceNormalization': AdaInstanceNormalization})\n","        mod.load_weights(\"Models/\"+name+\"_\"+str(num)+\".h5\")\n","        \n","        return mod\n","    \n","    def save(self, num): #Save JSON and Weights into /Models/\n","        self.saveModel(self.GAN.G, \"gen\", num)\n","        self.saveModel(self.GAN.D, \"dis\", num)\n","        \n","\n","    def load(self, num): #Load JSON and Weights from /Models/\n","        steps1 = self.GAN.steps\n","        \n","        self.GAN = None\n","        self.GAN = GAN()\n","\n","        #Load Models\n","        self.GAN.G = self.loadModel(\"gen\", num)\n","        self.GAN.D = self.loadModel(\"dis\", num)\n","        \n","        self.GAN.steps = steps1\n","        \n","        self.generator = self.GAN.generator()\n","        self.DisModel = self.GAN.DisModel()\n","        self.AdModel = self.GAN.AdModel()\n","        \n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIaxaQLy8R33","colab_type":"text"},"source":["# Run"]},{"cell_type":"code","metadata":{"id":"VVSOlsffIAzb","colab_type":"code","colab":{}},"source":["def visualize(num=0):\n","  temp = Image.open(\"Results/i\"+str(num)+\".jpg\")\n","  print(np.shape(temp))\n","  plt.imshow(temp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BcZ2_-Q8UpS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1a032745-5360-48fe-82e8-55c7c5e1f889"},"source":["if __name__ == \"__main__\":\n","    model = WGAN(lr = 0.0003, silent = False)\n","    \n","    for i in range (50):\n","      model.train()\n","    model.saveModel(model,\"final\",0)\n","    for i in range (50):\n","      model.train()\n","    model.saveModel(model,\"final\",1)\n","    for i in range (50):\n","      model.train()\n","    model.saveModel(model,\"final\",2)        \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mF-eKZdEJX1F","colab_type":"code","colab":{}},"source":["model.evaluate()\n","visualize()"],"execution_count":0,"outputs":[]}]}